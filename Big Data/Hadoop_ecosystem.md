## 빅데이터의 기준    

- 싱글 pc에서 처리할 수 없을 때

    `ex` 일반적인 pc의 경우, 16기가 RAM을 사용 

    ⇒ 32기가의 데이터 처리 필요     

    ⇒ 싱글 pc에서 처리 불가 

    ⇒ 빅데이터로 판단  

    `현업에서는...` 

    60테라바이트의 데이터를 처리하기 위해 3-40대 pc로 병렬 처리함. 즉, **빅데이터 분산 처리.**  

    하지만 기업이 아닌 이상 실제로 이정도의 데이터를 다루기 힘드므로, 일반적인 pc를 기준으로 하여 16 기가 스케일로 봤을 땐, 이보다 크다면 싱글 pc에서 처리할 수 없어 빅데이터로 볼 수 있다.   

    - 하루에 크롤링하는 신문 기사가 1200개 정도면, 텍스트로 1~2 기가 정도 예상.

        ⇒ 좋은 사이즈   
            
- 데이터(텍스트) 처리 방법
    - 배치 프로세스
        - 데이터를 한 번에 모아서 처리
        - 특화 프로젝트에서는 배치 프로세스로 진행할 확률이 높음
        
        `ex` MapReduce   
        
    - 스트리밍 프로세스
        - 들어올 때 마다 처리하므로 2,30 기가 데이터가 들어와도 처리 가능
## Hadoop ecosystem    

- HDFS : os로 따진다면, `파일 시스템`
- MapReduce : 가장 `기본적인 논리 연산` 시스템
    - 개발에서 가장 **로우레벨**의 언어인 C언어(=어셈블리어)부터 배워서 점점 올라가는 것처럼, MapReduce도 가장 **기본적인 것**으로, 개발로 따지면 C언어를 배우는 것.
- Kafka, Spark : `MapReduce를 기반`으로 만든 툴
    - 현업에선 Kafka, Spark, Flink 등 여러 배치 / 스트리밍 툴들이 제공되어 사용하는데, 요즘도 개발할 때 Java 등의 좀 더 **하이레벨의 언어**를 구사하는 것과 유사
    - MapReduce는 코드가 더러워서 좀 더 매니징되어 있는 Kafka, Spark를 쓴다고 보면 된다.
- Kafka
    - 스트리밍 서비스 툴 (=메세지 큐)
