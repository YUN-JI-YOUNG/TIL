# Web   
###### World Wide Web(WWW,W3)은 인터넷에 연결된 컴퓨터를 통해 사람들이 정보를 공유할 수 있는 전 세계적인 정보 공간   
> 컴퓨터간의 정보공유     

### 1. 웹의 구성요소   
- HTTP 프로토콜   
  - HTML 문서와 같은 리소스들을 가져올 수 있도록 해주는 프로토콜
  - 웹에스 이뤄지는 모든 데이터 교환의 기초
  - 서버 <-> 클라이언트간의 데이터 전송 관리    
  - 요청(requests) : 클라이언트에 의해 전송     
  - 응답(responses) : 서버에서 응답으로 전송      

  - URL (요청)   
    - 클라이언트가 웹 요소에 접근할 수 있게 함    
    - `?` : url <-> 파라미터 연결    
    - `&` : 파라미터 <-> 파라미터 연결     

  - 웹 문서 (응답)    
    - HTML 같은 웹 문서 작성    

- 대표적인 클라이언트 : 크롬, 엣지 등 브라우저  
  - 브라우저 외에 요청을 보낼 수 있는 클라이언트 : **requests 패키지**       
- 대표적인 서버 : 네이버, 구글 등 웹 서비스, GitHub    

</br>    

### 2. 크롤링   
###### 웹에서 원하는 정보를 가져오는 행위    

#### 2-1. requests 패키지 이용   
- Beautiful Soup - html에서 데이터를 뽑아내는 라이브러리
`Beautiful Soup(데이터, 'html_parser')` : html 문서 해석    
  - 파서 : 해석기     
  - 파싱 : 해석하는것     
  - 코드 외부에서 들어온 데이터는 모두 문자열이며, 딕셔너리처럼 보이는 것은 json str      
  -> 이걸 변환하는 작업이 해석(=파싱)       
  - 선택자 : html 문서에서 해당 데이터를 식별할 수 있게 해주는 문자       
  - 선택자 추출 방법 : 브라우저에서 원하는 데이터가 있는 페이지에서 마우스 오른쪽 클릭    
  -> 검사-> 해당 코드에서 마우스 오른쪽 클릭 -> copy -> copy selector          

```python 
response = requests.get(url)  # 실제 요청을 보내고 응답 객체를 변수에 저장     
response = requests.get('url').json()  # 응답 json str를 dict로 파싱 (가능한 데이터만 가능)      

data = BeautifulSoup(reponse.text, 'html.parser')  # 응답 객체의 본문(text)를 해석하여 변수에 저장  

kospi = data.select_one('{선택자}').text   # 선택자로 원하는 데이터 선택 후 본문만 변수에 저장   
```    

- 단점 : 브라우저가 아닌 상황에서 필요없는 데이터가 너무 많음 + 원하는 데이터를 위한 추가작업(선택자 찾는 등)     

</br>   

#### 2-2. API     
###### Application Programming Interface    
- 응용 프로그램에서 사용할 수 있도록 운영 체제나 프로그래밍 언어가 제공하는 기능을 제어할 수 있게 만든 인터페이스     
- 결국 인터페이스이므로 응용 프로그램을 위한 접점이라고도 할 수 있다.       
- requests 패키지의 단점을 극복하기 위한 방법이라고 할 수 있다.    
- API server : 응용 프로그램(개발자)를 위한 데이터를 응답하는 프로그램     
ex> agify - 이름기반으로 나이를 추측하는 API     
